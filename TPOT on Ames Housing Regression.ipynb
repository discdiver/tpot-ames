{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
      },
      "cell_type": "markdown",
      "source": "# TPOT Automated ML Exploration with Ames Housing Regression\n## By Jeff Hale\n\nLet's see how TPOT does with a regression task with minimal data preparation. See my [Medium article on TPOT](https://medium.com/p/4c063b3e5de9/) for more information."
    },
    {
      "metadata": {
        "_uuid": "c2d6471a1fc5051251672fe758e89acc55cff062"
      },
      "cell_type": "markdown",
      "source": "## Setup\nLet's import the libraries and methods we'll need and set some options to make data and charts display nicely."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ffac50c247bd1f28d3fab1502bea2d8ad4cc2fe0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nimport timeit\nimport category_encoders\nimport os\nfrom math import sqrt\nfrom sklearn.preprocessing import OneHotEncoder, Imputer, StandardScaler, MinMaxScaler, LabelEncoder, normalize\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV,  cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn_pandas import CategoricalImputer\nfrom tpot import TPOTRegressor\n\n%matplotlib inline\n\npd.set_option('display.max_rows', 20)\npd.set_option('display.max_columns', 300)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6402132698081d1665fb949a346cca054a84b63e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv(\"../input/train.csv\")\nprint(df.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "943f2a9694b591daeeae2175670d79ebef77cd34",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(df.info())\nprint(df.describe())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73b3b695fbb4f0ecfbc77c43bb16be36325704b7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# break into X and y dataframes\nX = df.reindex(columns=[x for x in df.columns.values if x != 'SalePrice'])        # separate out X\ny = df.reindex(columns=['SalePrice'])   # separate out y\ny = np.ravel(y)                     # flatten the y array\n\n# make list of numeric and string columns\nnumeric_cols = [] # could still have ordinal data\nstring_cols = []  # could have ordinal or nominal data\n\nfor col in X.columns:\n    if (X.dtypes[col] == np.int64 or X.dtypes[col] == np.int32 or X.dtypes[col] == np.float64):\n        numeric_cols.append(col)      # True integer or float columns\n    \n    if (X.dtypes[col] == np.object):  # Nominal and ordinal columns\n        string_cols.append(col)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73267bf881c3bd5f401375ce852bd1e8e2c7577a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(X[string_cols].head(2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "987e54cb87b6a70c055ec46117f445742722ad3e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# impute missing values for string columns using sklearnpandas CategoricalImputer for string data\n# s_imputer = CategoricalImputer(strategy=\"fixed_value\", replacement=\"missing\") \n# use above line as soon as sklearn-pandas updated\n# s_imputer = CategoricalImputer()\nX_string = X[string_cols]\n# print(type(X_string))\n# X_string = (s_imputer.fit_transform(X_string)\n\n# or X_string = X_string.apply(s_imputer.fit_transform)\n\n# X_string = pd.DataFrame(X_string, columns = string_cols)\nX_string = X_string.fillna(\"missing\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff14ea3767bed06602d62fbc4459e97a4c8fb86d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# encode the X columns string values as integers\nX_string = X_string.apply(LabelEncoder().fit_transform)  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78e819eaf88b4a83d155358a69aed89de9130e31",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(X.head(2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b9d97d2b2ff06b235701cf078c877cf90662af8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# imputing missing values with most freqent values for numeric columns\nn_imputer = Imputer(missing_values='NaN', copy = True, strategy = 'most_frequent') # imputing with most frequent because some of these numeric columns are ordinal\n\nX_numeric = X[numeric_cols]\nX_numeric = n_imputer.fit_transform(X_numeric)\nX_numeric = pd.DataFrame(X_numeric, columns = numeric_cols)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98d62a7d986690c56c73e8316e80ac5b1a20b06d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# add the string and numeric dataframes back together\nX = pd.concat([X_numeric, X_string], axis=1, join_axes=[X_numeric.index])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13ee3e3915f7733efc89720c3c89e9d2f987307a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "X.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a1df2c1893b3ac3e1bb156be54f1334fb2309cd8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# convert to numpy array so that if gets XGBoost algorithm doesn't throw \n# ValueError: feature_names mismatch: ...\n# see https://github.com/dmlc/xgboost/issues/2334\nX = X.values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa2680e42c2e8c858aaba9f3791248167c4488ec",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 22)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b4afbd2c113bec35bb50011a17d9270495750a4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Make a custom metric function for TPOT\n# Root mean squared logarithmic error is how Kaggle scores this task\n# Can't use custom scorer with n_jobs > 1.  Known issue.\n\n# def custom_rmsle(y_true, y_pred):\n#     return np.sqrt(np.mean((np.log(1 + y_pred) - np.log(1 + y_true))**2))\n\n# Make a custom scorer from the custom metric function\n# rmsle = make_scorer(custom_rmsle, greater_is_better=False)\n\n# Number of pipelines is very small below so that we can quickly commit on Kaggle\n\n# instantiate tpot \ntpot = TPOTRegressor(verbosity=3,  \n                    random_state=22, \n                    #scoring=rmsle,\n                    periodic_checkpoint_folder=\"any_string\",\n                    n_jobs=-1, \n                    warm_start = True,\n                    generations=3, \n                    population_size=5,\n                    early_stop=2)\ntimes = []\nscores = []\nwinning_pipes = []\n\n# run 2 iterations\nfor x in range(2):\n    start_time = timeit.default_timer()\n    tpot.fit(X_train, y_train)\n    elapsed = timeit.default_timer() - start_time\n    times.append(elapsed)\n    winning_pipes.append(tpot.fitted_pipeline_)\n    scores.append(tpot.score(X_test, y_test))\n    tpot.export('tpot_ames.py')\n\n# output results\ntimes = [time/60 for time in times]\nprint('Times:', times)\nprint('Scores:', scores)   \nprint('Winning pipelines:', winning_pipes)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}